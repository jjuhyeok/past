{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch_tabnet\n",
            "  Using cached pytorch_tabnet-4.0-py3-none-any.whl (41 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from pytorch_tabnet) (1.23.5)\n",
            "Requirement already satisfied: torch<2.0,>=1.2 in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from pytorch_tabnet) (1.13.1+cu117)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.36 in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from pytorch_tabnet) (4.64.1)\n",
            "Requirement already satisfied: scikit_learn>0.21 in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from pytorch_tabnet) (1.2.1)\n",
            "Requirement already satisfied: scipy>1.4 in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from pytorch_tabnet) (1.10.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from scikit_learn>0.21->pytorch_tabnet) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from scikit_learn>0.21->pytorch_tabnet) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from torch<2.0,>=1.2->pytorch_tabnet) (4.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from tqdm<5.0,>=4.36->pytorch_tabnet) (0.4.6)\n",
            "Installing collected packages: pytorch_tabnet\n",
            "Successfully installed pytorch_tabnet-4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_tabnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "yEnjW7OjGKcs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ineeji\\AppData\\Local\\Temp\\ipykernel_5684\\3633367802.py:19: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  drop_list = data.corr()['DSL D-95'].map(abs).sort_values(ascending = False)[10:]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 데이터 로드\n",
        "data = pd.read_csv('\\\\Users\\\\ineeji\\\\Desktop\\\\새 폴더\\\\Ineeji\\\\datas\\\\데이터합본_파생변수 제거.csv')\n",
        "data['year'] = data['Unnamed: 0'].apply(lambda x : x.split()[0].split('-')[0])\n",
        "data['month'] = data['Unnamed: 0'].apply(lambda x : x.split()[0].split('-')[1])\n",
        "data['date'] = data['Unnamed: 0'].apply(lambda x : x.split()[0].split('-')[2])\n",
        "data['hour'] = data['Unnamed: 0'].apply(lambda x : x.split()[1].split(' ')[0].split(':')[0])\n",
        "\n",
        "data['year'] = data['year'].astype('int')\n",
        "data['month'] = data['month'].astype('int')\n",
        "data['date'] = data['date'].astype('int')\n",
        "data['hour'] = data['hour'].astype('int')\n",
        "\n",
        "lower_out = data['DSL D-95'].mean() - data['DSL D-95'].std()*3\n",
        "upper_out = data['DSL D-95'].mean() + data['DSL D-95'].std()*3\n",
        "data = data[(data['DSL D-95'] > lower_out) & (data['DSL D-95'] < upper_out)]\n",
        "\n",
        "drop_list = data.corr()['DSL D-95'].map(abs).sort_values(ascending = False)[10:]\n",
        "drop_list = pd.DataFrame(drop_list)\n",
        "drop_list = drop_list.reset_index()\n",
        "drop_list = drop_list['index']\n",
        "\n",
        "train = data[(data['year'] == 2015) |(data['year'] == 2016) | (data['year'] == 2017) | (data['year'] == 2018)]\n",
        "test = data[(data['year'] == 2019) |(data['year'] == 2020) | (data['year'] == 2021)]\n",
        "\n",
        "train = train[train['hour'] == 7]\n",
        "test = test[test['hour'] == 7]\n",
        "test = test.reset_index(drop = True)\n",
        "train_x = train.drop(['DSL D-95'],axis=1)\n",
        "train_y = train['DSL D-95']\n",
        "test_x = test.drop(['DSL D-95'],axis=1)\n",
        "test_y = test['DSL D-95']\n",
        "\n",
        "train_x.drop(drop_list,axis=1,inplace=True)\n",
        "test_x.drop(drop_list,axis=1,inplace=True)\n",
        "train_x.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
        "test_x.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_x, train_y, test_size=0.2, random_state=9555)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "aGeUO8iXGySO"
      },
      "outputs": [],
      "source": [
        "# 모델 초기화\n",
        "#tabnet = TabNetRegressor(n_d=8, n_a=8, n_steps=3, gamma=1.3, lambda_sparse=0, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2), mask_type='entmax', device_name='auto')\n",
        "#8.7\n",
        "\n",
        "#tabnet = TabNetRegressor(n_d=16, n_a=16, n_steps=5, gamma=1.3, lambda_sparse=0, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2), mask_type='entmax', device_name='auto')\n",
        "#6.5\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "\n",
        "model = TabNetRegressor(n_d=8, n_a=8, n_steps=2, gamma=1.3, lambda_sparse=0, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2), mask_type='entmax', device_name=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "KU9X2AHVGsY3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 161723.26562| train_mse: 98310.99812| val_mse: 94706.67022|  0:00:00s\n",
            "epoch 1  | loss: 161482.90625| train_mse: 108692.22284| val_mse: 108393.21753|  0:00:00s\n",
            "epoch 2  | loss: 161215.8125| train_mse: 118709.45305| val_mse: 118581.3198|  0:00:00s\n",
            "epoch 3  | loss: 161102.51562| train_mse: 126377.96147| val_mse: 126200.25621|  0:00:00s\n",
            "epoch 4  | loss: 160925.21875| train_mse: 130009.10971| val_mse: 130817.44177|  0:00:00s\n",
            "epoch 5  | loss: 160650.32812| train_mse: 139155.00475| val_mse: 139837.43849|  0:00:00s\n",
            "epoch 6  | loss: 160385.03125| train_mse: 143783.30878| val_mse: 144207.9576|  0:00:00s\n",
            "epoch 7  | loss: 160161.40625| train_mse: 123859.8688| val_mse: 124199.32518|  0:00:00s\n",
            "epoch 8  | loss: 159974.0625| train_mse: 125740.67529| val_mse: 124843.49439|  0:00:01s\n",
            "epoch 9  | loss: 159509.26562| train_mse: 131108.58827| val_mse: 130165.9505|  0:00:01s\n",
            "epoch 10 | loss: 159229.79688| train_mse: 133697.03387| val_mse: 133529.68257|  0:00:01s\n",
            "epoch 11 | loss: 159031.40625| train_mse: 132402.34144| val_mse: 132698.79626|  0:00:01s\n",
            "epoch 12 | loss: 158886.0625| train_mse: 132413.84522| val_mse: 132505.5482|  0:00:01s\n",
            "epoch 13 | loss: 158400.25| train_mse: 133572.77096| val_mse: 133969.35963|  0:00:01s\n",
            "epoch 14 | loss: 158183.75| train_mse: 134009.08931| val_mse: 134602.31898|  0:00:01s\n",
            "epoch 15 | loss: 157847.9375| train_mse: 134483.07164| val_mse: 135175.89116|  0:00:01s\n",
            "epoch 16 | loss: 157471.21875| train_mse: 134555.12862| val_mse: 135283.94007|  0:00:01s\n",
            "epoch 17 | loss: 157026.3125| train_mse: 135095.76639| val_mse: 135810.90315|  0:00:01s\n",
            "epoch 18 | loss: 156662.21875| train_mse: 135755.96022| val_mse: 136242.43764|  0:00:01s\n",
            "epoch 19 | loss: 156184.5| train_mse: 136545.56669| val_mse: 137229.78454|  0:00:02s\n",
            "epoch 20 | loss: 155903.125| train_mse: 137401.83969| val_mse: 138248.89163|  0:00:02s\n",
            "epoch 21 | loss: 155509.15625| train_mse: 136960.94026| val_mse: 137268.67837|  0:00:02s\n",
            "epoch 22 | loss: 155179.09375| train_mse: 135280.95955| val_mse: 135378.76022|  0:00:02s\n",
            "epoch 23 | loss: 154646.40625| train_mse: 133357.38813| val_mse: 133311.06941|  0:00:02s\n",
            "epoch 24 | loss: 154096.03125| train_mse: 131856.58588| val_mse: 131534.1297|  0:00:02s\n",
            "epoch 25 | loss: 153665.57812| train_mse: 129544.80854| val_mse: 130050.18645|  0:00:02s\n",
            "epoch 26 | loss: 153163.34375| train_mse: 125270.59673| val_mse: 125244.37234|  0:00:02s\n",
            "epoch 27 | loss: 152816.34375| train_mse: 119615.90797| val_mse: 119938.14593|  0:00:02s\n",
            "epoch 28 | loss: 152269.48438| train_mse: 113634.54363| val_mse: 114448.11061|  0:00:02s\n",
            "epoch 29 | loss: 151898.60938| train_mse: 107765.3138| val_mse: 107681.45739|  0:00:02s\n",
            "epoch 30 | loss: 151368.3125| train_mse: 105179.13508| val_mse: 105291.69463|  0:00:03s\n",
            "epoch 31 | loss: 150882.71875| train_mse: 103175.42395| val_mse: 103031.48348|  0:00:03s\n",
            "epoch 32 | loss: 150308.73438| train_mse: 101681.316| val_mse: 101711.6648|  0:00:03s\n",
            "epoch 33 | loss: 149765.67188| train_mse: 100272.61728| val_mse: 100553.06224|  0:00:03s\n",
            "epoch 34 | loss: 149191.59375| train_mse: 99410.11957| val_mse: 99782.27937|  0:00:03s\n",
            "epoch 35 | loss: 148626.67188| train_mse: 98655.04559| val_mse: 99044.93047|  0:00:03s\n",
            "epoch 36 | loss: 148028.48438| train_mse: 97981.30048| val_mse: 98317.54093|  0:00:03s\n",
            "epoch 37 | loss: 147516.875| train_mse: 97215.47783| val_mse: 97352.36415|  0:00:03s\n",
            "epoch 38 | loss: 146859.21875| train_mse: 97491.34759| val_mse: 97514.69228|  0:00:03s\n",
            "epoch 39 | loss: 146362.1875| train_mse: 96269.98674| val_mse: 95796.94|  0:00:04s\n",
            "epoch 40 | loss: 145697.95312| train_mse: 95782.24932| val_mse: 95346.85185|  0:00:04s\n",
            "epoch 41 | loss: 145185.6875| train_mse: 94804.72841| val_mse: 94366.39787|  0:00:04s\n",
            "epoch 42 | loss: 144290.25| train_mse: 94236.57039| val_mse: 93513.6605|  0:00:04s\n",
            "epoch 43 | loss: 143771.73438| train_mse: 93962.5936| val_mse: 92999.24953|  0:00:04s\n",
            "epoch 44 | loss: 142980.28125| train_mse: 93774.92657| val_mse: 92722.22891|  0:00:04s\n",
            "epoch 45 | loss: 142451.5625| train_mse: 93059.17779| val_mse: 92145.78675|  0:00:04s\n",
            "epoch 46 | loss: 141750.21875| train_mse: 93038.69393| val_mse: 92102.95671|  0:00:04s\n",
            "epoch 47 | loss: 140965.75| train_mse: 93005.29761| val_mse: 92043.46291|  0:00:04s\n",
            "epoch 48 | loss: 140389.71875| train_mse: 93053.663| val_mse: 92106.29262|  0:00:04s\n",
            "epoch 49 | loss: 139587.03125| train_mse: 93197.91923| val_mse: 92396.92999|  0:00:05s\n",
            "epoch 50 | loss: 138896.40625| train_mse: 93242.6571| val_mse: 92732.70965|  0:00:05s\n",
            "epoch 51 | loss: 138139.95312| train_mse: 93546.23721| val_mse: 92948.17829|  0:00:05s\n",
            "epoch 52 | loss: 137360.46875| train_mse: 93791.57081| val_mse: 93153.24269|  0:00:05s\n",
            "epoch 53 | loss: 136670.375| train_mse: 93923.59547| val_mse: 93391.27729|  0:00:05s\n",
            "epoch 54 | loss: 135960.875| train_mse: 94214.75568| val_mse: 93744.34422|  0:00:05s\n",
            "epoch 55 | loss: 135157.90625| train_mse: 94430.81027| val_mse: 94032.93589|  0:00:05s\n",
            "epoch 56 | loss: 134435.71875| train_mse: 94373.72173| val_mse: 94063.07756|  0:00:05s\n",
            "epoch 57 | loss: 133620.96875| train_mse: 94215.11161| val_mse: 94030.00881|  0:00:05s\n",
            "epoch 58 | loss: 132884.125| train_mse: 94567.30356| val_mse: 94280.82505|  0:00:05s\n",
            "epoch 59 | loss: 132033.5| train_mse: 94733.19662| val_mse: 94416.30591|  0:00:05s\n",
            "epoch 60 | loss: 131148.03125| train_mse: 94985.73895| val_mse: 94649.58051|  0:00:06s\n",
            "epoch 61 | loss: 130380.75| train_mse: 95544.14831| val_mse: 95036.372|  0:00:06s\n",
            "epoch 62 | loss: 129401.29688| train_mse: 95789.97208| val_mse: 95288.72763|  0:00:06s\n",
            "epoch 63 | loss: 128661.39844| train_mse: 95348.50249| val_mse: 94884.04001|  0:00:06s\n",
            "epoch 64 | loss: 127795.86719| train_mse: 94921.25214| val_mse: 94552.74302|  0:00:06s\n",
            "epoch 65 | loss: 126879.28125| train_mse: 94494.06783| val_mse: 94218.56215|  0:00:06s\n",
            "epoch 66 | loss: 126121.60938| train_mse: 93835.67506| val_mse: 93612.63758|  0:00:06s\n",
            "epoch 67 | loss: 125110.70312| train_mse: 93272.1233| val_mse: 93080.35629|  0:00:06s\n",
            "epoch 68 | loss: 124311.625| train_mse: 92803.58712| val_mse: 92646.47065|  0:00:06s\n",
            "epoch 69 | loss: 123615.65625| train_mse: 92361.39171| val_mse: 92181.77069|  0:00:06s\n",
            "epoch 70 | loss: 122510.32031| train_mse: 91941.73836| val_mse: 91765.72991|  0:00:06s\n",
            "epoch 71 | loss: 121701.20312| train_mse: 91397.71273| val_mse: 91216.89058|  0:00:07s\n",
            "epoch 72 | loss: 120749.74219| train_mse: 90715.86864| val_mse: 90541.03636|  0:00:07s\n",
            "epoch 73 | loss: 119919.08594| train_mse: 90000.60922| val_mse: 89848.64396|  0:00:07s\n",
            "epoch 74 | loss: 118991.54688| train_mse: 89100.81835| val_mse: 89011.90574|  0:00:07s\n",
            "epoch 75 | loss: 118008.125| train_mse: 88278.4358| val_mse: 88237.78404|  0:00:07s\n",
            "epoch 76 | loss: 117203.20312| train_mse: 87302.61787| val_mse: 87320.0741|  0:00:07s\n",
            "epoch 77 | loss: 116243.48438| train_mse: 86218.02466| val_mse: 86303.39855|  0:00:07s\n",
            "epoch 78 | loss: 115236.91406| train_mse: 85403.89061| val_mse: 85557.52515|  0:00:07s\n",
            "epoch 79 | loss: 114355.03125| train_mse: 84651.38916| val_mse: 84825.12918|  0:00:07s\n",
            "epoch 80 | loss: 113395.67969| train_mse: 83903.78057| val_mse: 84109.2089|  0:00:08s\n",
            "epoch 81 | loss: 112514.85938| train_mse: 83251.87551| val_mse: 83493.21134|  0:00:08s\n",
            "epoch 82 | loss: 111528.24219| train_mse: 82537.44171| val_mse: 82811.47581|  0:00:08s\n",
            "epoch 83 | loss: 110502.76562| train_mse: 82036.58212| val_mse: 82275.45477|  0:00:08s\n",
            "epoch 84 | loss: 109510.14844| train_mse: 81481.73703| val_mse: 81729.00028|  0:00:08s\n",
            "epoch 85 | loss: 108454.55469| train_mse: 80964.38524| val_mse: 81241.34676|  0:00:08s\n",
            "epoch 86 | loss: 107416.90625| train_mse: 80522.00768| val_mse: 80831.86461|  0:00:08s\n",
            "epoch 87 | loss: 106453.76562| train_mse: 78628.18074| val_mse: 78929.43624|  0:00:08s\n",
            "epoch 88 | loss: 105497.16406| train_mse: 75050.7874| val_mse: 75285.35695|  0:00:08s\n",
            "epoch 89 | loss: 104406.8125| train_mse: 73002.50717| val_mse: 73246.35769|  0:00:08s\n",
            "epoch 90 | loss: 103440.125| train_mse: 73325.95867| val_mse: 73638.63345|  0:00:09s\n",
            "epoch 91 | loss: 102375.13281| train_mse: 73315.18566| val_mse: 73622.99064|  0:00:09s\n",
            "epoch 92 | loss: 101279.65625| train_mse: 72219.72096| val_mse: 72603.26832|  0:00:09s\n",
            "epoch 93 | loss: 100180.48438| train_mse: 70848.85381| val_mse: 71226.39951|  0:00:09s\n",
            "epoch 94 | loss: 99382.375| train_mse: 69143.12317| val_mse: 69492.92733|  0:00:09s\n",
            "epoch 95 | loss: 98142.78906| train_mse: 67535.25688| val_mse: 67939.53002|  0:00:09s\n",
            "epoch 96 | loss: 97136.41406| train_mse: 66016.33635| val_mse: 66401.88107|  0:00:09s\n",
            "epoch 97 | loss: 96100.83594| train_mse: 64862.42441| val_mse: 65214.62252|  0:00:09s\n",
            "epoch 98 | loss: 95016.35156| train_mse: 64314.88428| val_mse: 64819.91631|  0:00:09s\n",
            "epoch 99 | loss: 94020.78125| train_mse: 63764.08041| val_mse: 64252.81204|  0:00:09s\n",
            "epoch 100| loss: 92961.50781| train_mse: 62482.54292| val_mse: 62973.99744|  0:00:09s\n",
            "epoch 101| loss: 92015.67188| train_mse: 60760.25785| val_mse: 61172.12193|  0:00:10s\n",
            "epoch 102| loss: 90880.5 | train_mse: 60343.25252| val_mse: 60760.83844|  0:00:10s\n",
            "epoch 103| loss: 89870.11719| train_mse: 60102.38793| val_mse: 60553.9046|  0:00:10s\n",
            "epoch 104| loss: 88718.76562| train_mse: 59605.58126| val_mse: 60218.1544|  0:00:10s\n",
            "epoch 105| loss: 87550.60938| train_mse: 58874.63995| val_mse: 59544.46463|  0:00:10s\n",
            "epoch 106| loss: 86311.65625| train_mse: 57915.11908| val_mse: 58707.25774|  0:00:10s\n",
            "epoch 107| loss: 85184.38281| train_mse: 56951.68177| val_mse: 57819.31384|  0:00:10s\n",
            "epoch 108| loss: 84120.77344| train_mse: 56328.15049| val_mse: 57204.37617|  0:00:10s\n",
            "epoch 109| loss: 82811.40625| train_mse: 55730.40408| val_mse: 56665.41814|  0:00:10s\n",
            "epoch 110| loss: 81787.19531| train_mse: 56826.87588| val_mse: 57790.87491|  0:00:11s\n",
            "epoch 111| loss: 80645.95312| train_mse: 59010.34145| val_mse: 59151.6347|  0:00:11s\n",
            "epoch 112| loss: 79590.16406| train_mse: 59827.2873| val_mse: 60374.79395|  0:00:11s\n",
            "epoch 113| loss: 78349.875| train_mse: 59534.48772| val_mse: 60218.29281|  0:00:11s\n",
            "epoch 114| loss: 77261.46094| train_mse: 58447.91081| val_mse: 59318.63528|  0:00:11s\n",
            "epoch 115| loss: 76051.28906| train_mse: 58716.10294| val_mse: 59540.72893|  0:00:11s\n",
            "epoch 116| loss: 74940.53125| train_mse: 58448.06604| val_mse: 59213.62321|  0:00:11s\n",
            "epoch 117| loss: 73872.01562| train_mse: 58012.78536| val_mse: 58709.13407|  0:00:11s\n",
            "epoch 118| loss: 72666.39062| train_mse: 57187.5777| val_mse: 57758.38759|  0:00:11s\n",
            "epoch 119| loss: 71637.125| train_mse: 57156.56356| val_mse: 57605.96951|  0:00:11s\n",
            "epoch 120| loss: 70538.77344| train_mse: 57138.68234| val_mse: 57645.54343|  0:00:11s\n",
            "epoch 121| loss: 69419.70312| train_mse: 57154.88304| val_mse: 57552.15388|  0:00:11s\n",
            "epoch 122| loss: 68410.88281| train_mse: 56973.97983| val_mse: 57008.57029|  0:00:12s\n",
            "epoch 123| loss: 67221.64062| train_mse: 57647.51423| val_mse: 57486.97038|  0:00:12s\n",
            "epoch 124| loss: 66210.38281| train_mse: 59841.81273| val_mse: 59313.31082|  0:00:12s\n",
            "epoch 125| loss: 65016.60938| train_mse: 60149.23116| val_mse: 59126.75841|  0:00:12s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[141], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m y_valid_np \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(y_valid_np)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m \u001b[39m#model.fit( X_train_np, y_train_np, eval_set=[(X_train_np, y_train_np), (X_valid_np, y_valid_np)], eval_name=['train', 'val'], max_epochs=1000, patience=100)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m tabnet\u001b[39m.\u001b[39;49mfit( X_train\u001b[39m=\u001b[39;49mX_train\u001b[39m.\u001b[39;49mto_numpy(), y_train\u001b[39m=\u001b[39;49my_train\u001b[39m.\u001b[39;49mto_numpy()\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m), eval_set\u001b[39m=\u001b[39;49m[(X_train\u001b[39m.\u001b[39;49mto_numpy(), y_train\u001b[39m.\u001b[39;49mto_numpy()\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m)), (X_valid\u001b[39m.\u001b[39;49mto_numpy(), y_valid\u001b[39m.\u001b[39;49mto_numpy()\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m))], eval_name\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m'\u001b[39;49m], max_epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:245\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[39mfor\u001b[39;00m eval_name, valid_dataloader \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(eval_names, valid_dataloaders):\n\u001b[1;32m--> 245\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_epoch(eval_name, valid_dataloader)\n\u001b[0;32m    247\u001b[0m \u001b[39m# Call method on_epoch_end for all callbacks\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_epoch_end(\n\u001b[0;32m    249\u001b[0m     epoch_idx, logs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mepoch_metrics\n\u001b[0;32m    250\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:530\u001b[0m, in \u001b[0;36mTabModel._predict_epoch\u001b[1;34m(self, name, loader)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39m# Main loop\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[1;32m--> 530\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_batch(X)\n\u001b[0;32m    531\u001b[0m     list_y_true\u001b[39m.\u001b[39mappend(y)\n\u001b[0;32m    532\u001b[0m     list_y_score\u001b[39m.\u001b[39mappend(scores)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:558\u001b[0m, in \u001b[0;36mTabModel._predict_batch\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    555\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m    557\u001b[0m \u001b[39m# compute model output\u001b[39;00m\n\u001b[1;32m--> 558\u001b[0m scores, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(X)\n\u001b[0;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(scores, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    561\u001b[0m     scores \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m scores]\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:586\u001b[0m, in \u001b[0;36mTabNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    585\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder(x)\n\u001b[1;32m--> 586\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtabnet(x)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:472\u001b[0m, in \u001b[0;36mTabNetNoEmbeddings.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    470\u001b[0m res \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    471\u001b[0m steps_output, M_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)\n\u001b[1;32m--> 472\u001b[0m res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39;49mstack(steps_output, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_multi_task:\n\u001b[0;32m    475\u001b[0m     \u001b[39m# Result will be in list format\u001b[39;00m\n\u001b[0;32m    476\u001b[0m     out \u001b[39m=\u001b[39m []\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 학습\n",
        "X_train_np = X_train.values\n",
        "y_train_np = y_train.values\n",
        "X_valid_np = X_valid.values\n",
        "y_valid_np = y_valid.values\n",
        "y_train_np = y_train_np.reshape(-1, 1)\n",
        "y_valid_np = y_valid_np.reshape(-1, 1)\n",
        "\n",
        "X_train_np = torch.tensor(X_train_np).to(device)\n",
        "y_train_np = torch.tensor(y_train_np).to(device)\n",
        "X_valid_np = torch.tensor(X_valid_np).to(device)\n",
        "y_valid_np = torch.tensor(y_valid_np).to(device)\n",
        "\n",
        "#model.fit( X_train_np, y_train_np, eval_set=[(X_train_np, y_train_np), (X_valid_np, y_valid_np)], eval_name=['train', 'val'], max_epochs=1000, patience=100)\n",
        "\n",
        "model.fit( X_train=X_train.to_numpy(), y_train=y_train.to_numpy().reshape(-1,1), eval_set=[(X_train.to_numpy(), y_train.to_numpy().reshape(-1,1)), (X_valid.to_numpy(), y_valid.to_numpy().reshape(-1,1))], eval_name=['train', 'val'], max_epochs=1000, patience=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "abc = model.device\n",
        "abc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "Z7phelWkGwF3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5.331071732802701"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 예측\n",
        "test_x_np = test_x.values\n",
        "\n",
        "y_pred = model.predict(test_x_np)\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "score = mean_absolute_error(test_y,y_pred)\n",
        "score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "kBDzZ9RUH4ag"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "48.704076049526314"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "score = mean_absolute_error(test_y,y_pred)\n",
        "score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.26842032, 0.03063448, 0.21317795, 0.28828022, 0.01125697,\n",
              "       0.01811744, 0.08206685, 0.02498009, 0.06306569])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "importance = tabnet.feature_importances_\n",
        "importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip uninstall pytorch_tabnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_tabnet in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (4.0)\n",
            "Requirement already satisfied: torch<2.0,>=1.2 in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from pytorch_tabnet) (1.13.1+cu117)\n",
            "Requirement already satisfied: scipy>1.4 in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from pytorch_tabnet) (1.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from pytorch_tabnet) (1.23.5)\n",
            "Requirement already satisfied: scikit_learn>0.21 in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from pytorch_tabnet) (1.2.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.36 in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from pytorch_tabnet) (4.64.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from scikit_learn>0.21->pytorch_tabnet) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from scikit_learn>0.21->pytorch_tabnet) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from torch<2.0,>=1.2->pytorch_tabnet) (4.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\ineeji\\anaconda3\\envs\\torch_1\\lib\\site-packages (from tqdm<5.0,>=4.36->pytorch_tabnet) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pytorch_tabnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'pytorch_tabnet.utils' has no attribute 'explain'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[28], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m feat_importances \u001b[39m=\u001b[39m tabnet\u001b[39m.\u001b[39mfeature_importances_\n\u001b[0;32m      4\u001b[0m fig, axs \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m8\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m utils\u001b[39m.\u001b[39;49mexplain\u001b[39m.\u001b[39mplot_feature_importance(feat_importances, ax\u001b[39m=\u001b[39maxs)\n\u001b[0;32m      6\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'pytorch_tabnet.utils' has no attribute 'explain'"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pytorch_tabnet.utils as utils\n",
        "import matplotlib.pyplot as plt\n",
        "feat_importances = tabnet.feature_importances_\n",
        "fig, axs = plt.subplots(1, 1, figsize=(10, 8))\n",
        "utils.explain.plot_feature_importance(feat_importances, ax=axs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "475a4d6b143a342d14e19daf07a290e5b616d2baefab9a32fbee377de4d85fd4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
